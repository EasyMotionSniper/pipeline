ä¸ºäº†å®ç°é«˜æ•ˆçš„å¤šè·¯å½’å¹¶å¤„ç†S3ä¸Šçš„Parquetæ–‡ä»¶ï¼ŒåŒæ—¶ç»“åˆç£ç›˜ç¼“å†²ã€LRUç¼“å­˜å’Œå½’å¹¶ç®—æ³•ä¼˜åŒ–ååé‡ï¼Œä»¥ä¸‹æ˜¯ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆï¼š

âš™ï¸ ä¸€ã€æ¶æ„è®¾è®¡

1. æ•°æ®æµåˆ†å±‚  
   â€¢ S3å±‚ï¼šå­˜å‚¨åŸå§‹Parquetæ–‡ä»¶ï¼ˆæ¯ä¸ªæ–‡ä»¶å†…éƒ¨æŒ‰æŸåˆ—æœ‰åºï¼‰ã€‚  

   â€¢ ç£ç›˜ç¼“å†²å±‚ï¼šLRUç®¡ç†è¿‘æœŸè®¿é—®çš„æ–‡ä»¶æ•°æ®å—ï¼Œå‡å°‘é‡å¤ä¸‹è½½ã€‚  

   â€¢ å†…å­˜å½’å¹¶å±‚ï¼šå¤šè·¯å½’å¹¶å™¨ä»å„æ–‡ä»¶æµä¸­æŒ‰åºæå–æ•°æ®ï¼Œåˆå¹¶ä¸ºå…¨å±€æœ‰åºåºåˆ—ã€‚  

2. æ ¸å¿ƒç»„ä»¶  
   graph LR
   S3[Parquetæ–‡ä»¶] -->|å¼‚æ­¥ä¸‹è½½| DiskCache[ç£ç›˜ç¼“å†²]
   DiskCache -->|æŒ‰éœ€åŠ è½½| MergeCore[å¤šè·¯å½’å¹¶å™¨]
   MergeCore -->|è¾“å‡º| UserFunc[ç”¨æˆ·å›è°ƒ]
   

ğŸš€ äºŒã€å…³é”®å®ç°ä¸ä¼˜åŒ–

1. S3é«˜æ•ˆè¯»å–

â€¢ åˆ†å—å¼‚æ­¥ä¸‹è½½  

  ä½¿ç”¨AWS C++ SDKçš„åˆ†å—è¯»å–æ¥å£ï¼Œé¿å…å•æ¬¡å¤§æ–‡ä»¶ä¸‹è½½é˜»å¡ï¼Œç»“åˆå¼‚æ­¥IOæœ€å¤§åŒ–å¸¦å®½åˆ©ç”¨ç‡ï¼š  
  Aws::S3::Model::GetObjectRequest request;
  request.SetBucket(bucket_name);
  request.SetKey(object_key);
  request.SetRange("bytes=0-65535"); // æŒ‰éœ€åˆ†å—
  auto outcome = s3_client.GetObjectAsync(request, callback);
  
â€¢ åˆ—å¼è¯»å–ä¼˜åŒ–  

  ä»…è¯»å–å½’å¹¶æ‰€éœ€çš„åˆ—ï¼ˆå¦‚æ’åºé”®å’Œç”¨æˆ·æŒ‡å®šå­—æ®µï¼‰ï¼Œå‡å°‘I/Oé‡ï¼ˆParquetåˆ—å¼å­˜å‚¨ä¼˜åŠ¿ï¼‰ã€‚

2. ç£ç›˜ç¼“å†²ç®¡ç†ï¼ˆLRUç­–ç•¥ï¼‰

â€¢ æ•°æ®ç»“æ„ï¼šstd::list + std::unordered_map å®ç°O(1)æ“ä½œï¼š  
  class DiskCache {
      std::list<FileBlock> lru_list; // åŒå‘é“¾è¡¨ç»´æŠ¤è®¿é—®é¡ºåº
      std::unordered_map<std::string, typename std::list<FileBlock>::iterator> cache_map;
      size_t capacity;
      
      void put(const std::string& block_id, const FileBlock& block) {
          if (cache_map.size() >= capacity) {
              auto last = lru_list.back();
              cache_map.erase(last.id);
              lru_list.pop_back();
          }
          lru_list.push_front(block);
          cache_map[block_id] = lru_list.begin();
      }
  };
  
â€¢ æ–‡ä»¶å—ç®¡ç†  

  å°†Parquetæ–‡ä»¶æŒ‰å›ºå®šå¤§å°ï¼ˆå¦‚4MBï¼‰åˆ†å—å­˜å‚¨ï¼ŒLRUæ·˜æ±°æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„å—ã€‚

3. å¤šè·¯å½’å¹¶ç®—æ³•ä¼˜åŒ–

â€¢ è´¥è€…æ ‘ï¼ˆLoser Treeï¼‰  

  é€‚ç”¨äºå¤§è§„æ¨¡å¤–éƒ¨æ’åºï¼Œå‡å°‘æ¯”è¾ƒæ¬¡æ•°ï¼ˆæ—¶é—´å¤æ‚åº¦O(n log k)ï¼‰ï¼š  
  class LoserTree {
      std::vector<int> tree; // è´¥è€…æ ‘å†…éƒ¨èŠ‚ç‚¹
      std::vector<FileIterator> iters; // æ–‡ä»¶è¿­ä»£å™¨
      
      void adjust(int idx) {
          // æ›´æ–°è·¯å¾„ï¼Œé‡æ–°æ¯”è¾ƒè´¥è€…
      }
  public:
      void init(const std::vector<FileIterator>& sources) { /* åˆå§‹åŒ–æ ‘ */ }
      Record next() { 
          // è¾“å‡ºå½“å‰æœ€å°è®°å½•ï¼Œæ¨è¿›å¯¹åº”è¿­ä»£å™¨
          // è°ƒç”¨adjustæ›´æ–°æ ‘
      }
  };
  
â€¢ è¿­ä»£å™¨è®¾è®¡  

  æ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªè¿­ä»£å™¨ï¼Œå°è£…ç£ç›˜ç¼“å†²è¯»å–å’ŒParquetè§£æï¼š  
  class FileIterator {
      ParquetReader reader;
      DiskCache* cache;
      Record current;
      bool next() {
          // ä»ç¼“å­˜åŠ è½½ä¸‹ä¸€å—ï¼Œè§£æåæ›´æ–°current
      }
  };
  

4. å½’å¹¶è¿‡ç¨‹æµæ°´çº¿

â€¢ åŒç¼“å†²æŠ€æœ¯  

  é¢„åŠ è½½ä¸‹ä¸€æ‰¹æ•°æ®å—ï¼Œéšè—I/Oå»¶è¿Ÿï¼š  
  std::vector<Record> buffer1, buffer2;
  std::thread loader([&] { 
      while (has_next) buffer2.push_back(load_next()); 
  });
  // å½’å¹¶å½“å‰buffer1æ—¶ï¼Œåå°å¡«å……buffer2
  std::swap(buffer1, buffer2);
  

âš¡ï¸ ä¸‰ã€æ€§èƒ½æå‡ç­–ç•¥

1. åˆ—è£å‰ªä¸è°“è¯ä¸‹æ¨  
   åœ¨S3è¯»å–é˜¶æ®µè¿‡æ»¤æ— å…³åˆ—ï¼Œå‡å°‘ä¼ è¾“å’Œè§£æå¼€é”€ã€‚
2. å½’å¹¶å¹¶è¡ŒåŒ–  
   â€¢ å¤šçº¿ç¨‹è§£æä¸åŒæ–‡ä»¶å—ã€‚  

   â€¢ å½’å¹¶æ ‘åˆ†å±‚åˆå¹¶ï¼ˆå…ˆä¸¤ä¸¤å½’å¹¶ï¼Œå†åˆå¹¶ç»“æœï¼‰ã€‚

3. å†…å­˜æ˜ å°„æ–‡ä»¶  
   å¯¹é¢‘ç¹è®¿é—®çš„ç¼“å†²æ–‡ä»¶ä½¿ç”¨mmapï¼Œé¿å…å†…æ ¸åˆ°ç”¨æˆ·ç©ºé—´æ‹·è´ã€‚
4. ç½‘ç»œä¼˜åŒ–  
   â€¢ å¯ç”¨S3ä¼ è¾“åŠ é€Ÿï¼ˆTransfer Accelerationï¼‰ã€‚  

   â€¢ è°ƒæ•´TCPçª—å£å¤§å°å’Œå¹¶å‘è¿æ¥æ•°ã€‚

ğŸ§© å››ã€å®Œæ•´å·¥ä½œæµç¨‹

1. åˆå§‹åŒ–  
   â€¢ åŠ è½½ç”¨æˆ·æŒ‡å®šçš„S3æ–‡ä»¶åˆ—è¡¨ã€‚  

   â€¢ åˆå§‹åŒ–LRUç£ç›˜ç¼“å­˜ï¼ˆå®¹é‡æ ¹æ®ç£ç›˜ç©ºé—´é…ç½®ï¼‰ã€‚  

   â€¢ ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºè¿­ä»£å™¨ï¼Œé¢„è¯»ç¬¬ä¸€å—æ•°æ®ã€‚

2. æ„å»ºå½’å¹¶ç»“æ„  
   std::vector<FileIterator> iters;
   for (auto& file : files) {
       iters.push_back(FileIterator(file, disk_cache));
   }
   LoserTree merger(iters); // åˆå§‹åŒ–è´¥è€…æ ‘
   

3. æµå¼å½’å¹¶  
   while (merger.has_next()) {
       Record record = merger.next();
       user_callback(record); // ç”¨æˆ·å¤„ç†å‡½æ•°
   }
   

4. ç¼“å­˜ä¸IOååŒ  
   â€¢ è¿­ä»£å™¨ç¼ºæ•°æ®æ—¶è§¦å‘ç¼“å­˜åŠ è½½ã€‚  

   â€¢ ç¼“å­˜æœªå‘½ä¸­åˆ™ä»S3å¼‚æ­¥ä¸‹è½½ã€‚

âš ï¸ äº”ã€æ³¨æ„äº‹é¡¹

1. é”™è¯¯å¤„ç†  
   â€¢ æ ¡éªŒParquetæ–‡ä»¶å…ƒæ•°æ®ä¸€è‡´æ€§ã€‚  

   â€¢ S3ä¸‹è½½å¤±è´¥é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰ã€‚

2. èµ„æºæ§åˆ¶  
   â€¢ é™åˆ¶å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°ã€‚  

   â€¢ ç›‘æ§å†…å­˜/ç£ç›˜ä½¿ç”¨ï¼Œé˜²æ­¢æº¢å‡ºã€‚

3. çº¿ç¨‹å®‰å…¨  
   â€¢ ç¼“å­˜æ“ä½œéœ€åŠ é”ï¼ˆå¦‚std::mutexï¼‰ã€‚  

   â€¢ å½’å¹¶å™¨å†…éƒ¨çŠ¶æ€é¿å…ç«äº‰ã€‚

ğŸ’ æ€»ç»“

é€šè¿‡ S3åˆ†å—å¼‚æ­¥ä¸‹è½½ï¼ˆå‡å°‘å»¶è¿Ÿï¼‰ã€LRUç£ç›˜ç¼“å­˜ï¼ˆé¿å…é‡å¤IOï¼‰ã€è´¥è€…æ ‘å½’å¹¶ï¼ˆé«˜æ•ˆåˆå¹¶ï¼‰å’Œ åˆ—å¼è¯»å–ï¼ˆæœ€å°åŒ–ä¼ è¾“é‡ï¼‰ï¼Œå¯æ˜¾è‘—æå‡ååé‡ã€‚å…³é”®ä¼˜åŒ–ç‚¹ï¼š  
â€¢ ä¼˜å…ˆä½¿ç”¨ è´¥è€…æ ‘ è€Œéæœ€å°å †ï¼ˆæ¯”è¾ƒæ¬¡æ•°æ›´å°‘ï¼‰ã€‚  

â€¢ åŒç¼“å†²+æµæ°´çº¿ éšè—I/Oå»¶è¿Ÿã€‚  

â€¢ LRUç¼“å­˜å—ç²’åº¦ éœ€åŒ¹é…Parquetè¡Œç»„å¤§å°ï¼ˆé€šå¸¸128MBï¼‰ã€‚  

æœ€ç»ˆæ€§èƒ½å–å†³äºç½‘ç»œå¸¦å®½ã€ç£ç›˜é€Ÿåº¦å’Œå½’å¹¶å¤æ‚åº¦ã€‚å®æµ‹ä¸­å»ºè®®ç»“åˆAWS SDKæ—¥å¿—å’Œæ€§èƒ½åˆ†æå·¥å…·ï¼ˆå¦‚perfï¼‰æŒç»­è°ƒä¼˜ã€‚

#include "s3_parquet_merger.h"
#include <iostream>
#include <vector>

int main() {
    try {
        // åˆ›å»ºå½’å¹¶å™¨å®ä¾‹ï¼Œè®¾ç½®ç¼“å­˜å¤§å°ä¸º10000æ¡è®°å½•ï¼Œä½¿ç”¨4ä¸ªå·¥ä½œçº¿ç¨‹
        S3ParquetMerger merger(10000, 4, "./s3_cache");
        
        // æ·»åŠ éœ€è¦åˆå¹¶çš„S3æ–‡ä»¶è·¯å¾„
        std::vector<std::string> s3_files = {
            "my-bucket/data/file1.parquet",
            "my-bucket/data/file2.parquet",
            "my-bucket/data/file3.parquet"
            // å¯ä»¥æ·»åŠ æ›´å¤šæ–‡ä»¶...
        };
        merger.add_files(s3_files);
        
        // å‡†å¤‡åˆå¹¶ï¼šåŠ è½½æ–‡ä»¶å¹¶åˆå§‹åŒ–å½’å¹¶é˜Ÿåˆ—
        std::cout << "Preparing merge..." << std::endl;
        merger.prepare_merge();
        
        // è¯»å–å¹¶å¤„ç†åˆå¹¶åçš„è®°å½•
        std::cout << "Starting to merge..." << std::endl;
        DataRecord record;
        size_t count = 0;
        while (merger.get_next_record(record)) {
            // å¤„ç†è®°å½•
            // std::cout << "Key: " << record.key << std::endl;
            
            count++;
            if (count % 1000 == 0) {
                std::cout << "Processed " << count << " records..." << std::endl;
            }
        }
        
        std::cout << "Merge completed. Total records processed: " << count << std::endl;
        
        // æ¸…ç†ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        merger.clean_cache();
    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}

#include "s3_parquet_merger.h"
#include <aws/core/Aws.h>
#include <aws/core/utils/Outcome.h>
#include <aws/s3/model/GetObjectRequest.h>
#include <aws/s3/model/ListObjectsRequest.h>
#include <parquet/stream_reader.h>
#include <fstream>
#include <iostream>
#include <chrono>
#include <algorithm>
#include <filesystem>

namespace fs = std::filesystem;

S3ParquetMerger::S3ParquetMerger(size_t cache_size, size_t threads, const std::string& cache_directory)
    : cache_capacity(cache_size), thread_count(threads), current_timestamp(0), 
      cache_dir(cache_directory), stop_workers(false) {
    
    // åˆå§‹åŒ–AWS SDK
    Aws::SDKOptions options;
    Aws::InitAPI(options);
    
    // åˆ›å»ºS3å®¢æˆ·ç«¯
    s3_client = std::make_shared<aws::s3::S3Client>();
    
    // åˆ›å»ºç¼“å­˜ç›®å½•
    fs::create_directories(cache_dir);
    
    // åˆå§‹åŒ–çº¿ç¨‹æ± 
    for (size_t i = 0; i < thread_count; ++i) {
        workers.emplace_back(&S3ParquetMerger::worker_thread, this);
    }
}

S3ParquetMerger::~S3ParquetMerger() {
    // åœæ­¢å·¥ä½œçº¿ç¨‹
    {
        std::lock_guard<std::mutex> lock(task_mutex);
        stop_workers = true;
    }
    task_cv.notify_all();
    for (auto& worker : workers) {
        worker.join();
    }
    
    // å…³é—­AWS SDK
    Aws::ShutdownAPI({});
}

void S3ParquetMerger::add_files(const std::vector<std::string>& s3_paths) {
    for (const auto& path : s3_paths) {
        FileMetadata meta;
        meta.s3_path = path;
        meta.local_path = cache_dir + "/" + std::to_string(std::hash<std::string>{}(path)) + ".parquet";
        meta.is_loaded = false;
        files.push_back(meta);
    }
}

void S3ParquetMerger::prepare_merge() {
    source_positions.resize(files.size(), 0);
    source_records.resize(files.size());
    
    // å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ–‡ä»¶çš„ç¬¬ä¸€æ‰¹æ•°æ®
    std::vector<std::future<void>> futures;
    
    for (size_t i = 0; i < files.size(); ++i) {
        futures.emplace_back(std::async(std::launch::async, [this, i]() {
            source_records[i] = get_file_records(files[i].s3_path);
            files[i].is_loaded = true;
            
            // å°†æ¯ä¸ªæºçš„ç¬¬ä¸€ä¸ªè®°å½•åŠ å…¥ä¼˜å…ˆé˜Ÿåˆ—
            if (!source_records[i].empty()) {
                std::lock_guard<std::mutex> lock(task_mutex);
                merge_queue.push({source_records[i][0], i, 0});
            }
        }));
    }
    
    // ç­‰å¾…æ‰€æœ‰æ–‡ä»¶åŠ è½½å®Œæˆ
    for (auto& future : futures) {
        future.get();
    }
}

bool S3ParquetMerger::get_next_record(DataRecord& record) {
    if (merge_queue.empty()) {
        return false;
    }
    
    // è·å–å½“å‰æœ€å°å…ƒç´ 
    auto current = merge_queue.top();
    merge_queue.pop();
    record = current.record;
    
    // ä»åŒä¸€æºè·å–ä¸‹ä¸€ä¸ªè®°å½•
    size_t next_index = current.record_index + 1;
    if (next_index < source_records[current.source_index].size()) {
        // ç›´æ¥ä»å†…å­˜ä¸­è·å–
        merge_queue.push({
            source_records[current.source_index][next_index],
            current.source_index,
            next_index
        });
    } else {
        // è¯¥æºæ•°æ®å·²ç”¨å®Œï¼Œå¯ä»¥è€ƒè™‘å¼‚æ­¥é¢„åŠ è½½æ›´å¤šæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        // è¿™é‡Œå‡è®¾æ–‡ä»¶æ˜¯å®Œæ•´åŠ è½½çš„ï¼Œå®é™…ä¸­å¯èƒ½éœ€è¦åˆ†ç‰‡åŠ è½½
    }
    
    return true;
}

void S3ParquetMerger::clean_cache() {
    // æ¸…ç†ç£ç›˜ç¼“å­˜
    if (fs::exists(cache_dir)) {
        fs::remove_all(cache_dir);
        fs::create_directories(cache_dir);
    }
    
    // æ¸…ç©ºå†…å­˜ç¼“å­˜
    lru_cache.clear();
}

bool S3ParquetMerger::download_file(const std::string& s3_path, const std::string& local_path) {
    // è§£æS3è·¯å¾„ï¼ˆå‡è®¾æ ¼å¼ä¸º "bucket/key"ï¼‰
    size_t slash_pos = s3_path.find('/');
    if (slash_pos == std::string::npos) {
        std::cerr << "Invalid S3 path: " << s3_path << std::endl;
        return false;
    }
    
    std::string bucket = s3_path.substr(0, slash_pos);
    std::string key = s3_path.substr(slash_pos + 1);
    
    Aws::S3::Model::GetObjectRequest request;
    request.SetBucket(bucket);
    request.SetKey(key);
    
    auto outcome = s3_client->GetObject(request);
    if (!outcome.IsSuccess()) {
        std::cerr << "Error downloading " << s3_path << ": " 
                  << outcome.GetError().GetMessage() << std::endl;
        return false;
    }
    
    // å°†æ–‡ä»¶å†…å®¹å†™å…¥æœ¬åœ°ç¼“å­˜
    std::ofstream outfile(local_path, std::ios::binary);
    outfile << outcome.GetResult().GetBody().rdbuf();
    
    return true;
}

std::vector<DataRecord> S3ParquetMerger::parse_parquet_file(const std::string& file_path) {
    std::vector<DataRecord> records;
    
    try {
        // æ‰“å¼€Parquetæ–‡ä»¶
        std::shared_ptr<parquet::RandomAccessFile> file = 
            parquet::ParquetFileReader::OpenFile(file_path, false);
        
        // åˆ›å»ºParquetæ–‡ä»¶é˜…è¯»å™¨
        std::unique_ptr<parquet::ParquetFileReader> reader = 
            parquet::ParquetFileReader::Open(file);
        
        // è·å–æ–‡ä»¶ä¸­çš„åˆ—æ•°å’Œè¡Œæ•°
        int num_row_groups = reader->num_row_groups();
        
        // è¯»å–æ‰€æœ‰è¡Œç»„
        for (int rg = 0; rg < num_row_groups; ++rg) {
            std::shared_ptr<parquet::RowGroupReader> row_group = reader->RowGroup(rg);
            
            // å‡è®¾æˆ‘ä»¬çŸ¥é“schemaï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
            // å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å®é™…schemaè§£æ
            parquet::StreamReader stream_reader{row_group};
            
            DataRecord record;
            // å‡è®¾ç¬¬ä¸€ä¸ªå­—æ®µæ˜¯keyï¼Œå…¶ä½™æ˜¯æ•°æ®
            while (!stream_reader.eof()) {
                stream_reader >> record.key;
                // è¯»å–å…¶ä»–å­—æ®µ...
                
                records.push_back(record);
            }
        }
    } catch (const std::exception& e) {
        std::cerr << "Error parsing Parquet file: " << e.what() << std::endl;
    }
    
    return records;
}

std::vector<DataRecord> S3ParquetMerger::get_file_records(const std::string& s3_path) {
    std::vector<DataRecord> records;
    
    // å…ˆæ£€æŸ¥LRUç¼“å­˜
    if (lru_cache_get(s3_path, records)) {
        return records;
    }
    
    // æŸ¥æ‰¾æ–‡ä»¶å…ƒæ•°æ®
    FileMetadata* meta = nullptr;
    for (auto& m : files) {
        if (m.s3_path == s3_path) {
            meta = &m;
            break;
        }
    }
    
    if (!meta) {
        std::cerr << "File not found in metadata: " << s3_path << std::endl;
        return records;
    }
    
    // æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
    bool file_exists = fs::exists(meta->local_path);
    
    // å¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼Œä»S3ä¸‹è½½
    if (!file_exists) {
        if (!download_file(s3_path, meta->local_path)) {
            std::cerr << "Failed to download file: " << s3_path << std::endl;
            return records;
        }
    }
    
    // è§£æParquetæ–‡ä»¶
    records = parse_parquet_file(meta->local_path);
    
    // æ”¾å…¥LRUç¼“å­˜
    lru_cache_put(s3_path, records);
    
    return records;
}

void S3ParquetMerger::lru_cache_put(const std::string& key, const std::vector<DataRecord>& records) {
    std::lock_guard<std::mutex> lock(task_mutex);
    
    // å¦‚æœç¼“å­˜å·²æ»¡ï¼Œå…ˆé©±é€æœ€å°‘ä½¿ç”¨çš„é¡¹
    while (lru_cache.size() >= cache_capacity) {
        lru_evict();
    }
    
    // æ·»åŠ æ–°é¡¹
    lru_cache[key] = {records, current_timestamp++, false};
}

bool S3ParquetMerger::lru_cache_get(const std::string& key, std::vector<DataRecord>& records) {
    std::lock_guard<std::mutex> lock(task_mutex);
    
    auto it = lru_cache.find(key);
    if (it == lru_cache.end()) {
        return false;
    }
    
    // æ›´æ–°è®¿é—®æ—¶é—´æˆ³
    it->second.last_accessed = current_timestamp++;
    records = it->second.records;
    return true;
}

void S3ParquetMerger::lru_evict() {
    if (lru_cache.empty()) return;
    
    // æ‰¾åˆ°æœ€å°‘ä½¿ç”¨çš„é¡¹
    auto least_used = lru_cache.begin();
    for (auto it = lru_cache.begin(); it != lru_cache.end(); ++it) {
        if (it->second.last_accessed < least_used->second.last_accessed) {
            least_used = it;
        }
    }
    
    // ç§»é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
    lru_cache.erase(least_used);
}

void S3ParquetMerger::worker_thread() {
    while (true) {
        std::function<void()> task;
        
        // è·å–ä»»åŠ¡
        {
            std::unique_lock<std::mutex> lock(task_mutex);
            task_cv.wait(lock, [this] { return stop_workers || !tasks.empty(); });
            
            if (stop_workers && tasks.empty()) {
                return;
            }
            
            task = std::move(tasks.front());
            tasks.pop();
        }
        
        // æ‰§è¡Œä»»åŠ¡
        task();
    }
}

void S3ParquetMerger::enqueue_task(const std::function<void()>& task) {
    {
        std::lock_guard<std::mutex> lock(task_mutex);
        tasks.push(task);
    }
    task_cv.notify_one();
}


#ifndef S3_PARQUET_MERGER_H
#define S3_PARQUET_MERGER_H

#include <aws/s3/S3Client.h>
#include <parquet/file_reader.h>
#include <memory>
#include <vector>
#include <queue>
#include <string>
#include <unordered_map>
#include <mutex>
#include <thread>
#include <condition_variable>
#include <future>
#include <fstream>
#include <filesystem>

// å®šä¹‰æ•°æ®è®°å½•ç»“æ„
struct DataRecord {
    std::string key;
    // å…¶ä»–å­—æ®µ...
    std::vector<uint8_t> data;
    
    // ç”¨äºæ¯”è¾ƒçš„è¿ç®—ç¬¦
    bool operator<(const DataRecord& other) const {
        return key < other.key;
    }
    
    bool operator>(const DataRecord& other) const {
        return key > other.key;
    }
};

// ç¼“å­˜é¡¹ç»“æ„
struct CacheItem {
    std::vector<DataRecord> records;
    size_t last_accessed;  // ç”¨äºLRU
    bool dirty;            // æ˜¯å¦è¢«ä¿®æ”¹
};

// æ–‡ä»¶å…ƒæ•°æ®
struct FileMetadata {
    std::string s3_path;
    std::string local_path;
    size_t file_size;
    bool is_loaded;
};

// ç”¨äºä¼˜å…ˆé˜Ÿåˆ—çš„å…ƒç´ 
struct QueueElement {
    DataRecord record;
    size_t source_index;
    size_t record_index;
    
    bool operator<(const QueueElement& other) const {
        return record > other.record;  // ä½¿ä¼˜å…ˆé˜Ÿåˆ—æˆä¸ºæœ€å°å †
    }
};

class S3ParquetMerger {
private:
    std::shared_ptr<aws::s3::S3Client> s3_client;
    std::vector<FileMetadata> files;
    std::unordered_map<std::string, CacheItem> lru_cache;
    size_t cache_capacity;  // ç¼“å­˜å®¹é‡(è®°å½•æ•°)
    size_t current_timestamp;
    std::string cache_dir;  // ç£ç›˜ç¼“å­˜ç›®å½•
    
    // çº¿ç¨‹æ± ç›¸å…³
    size_t thread_count;
    std::vector<std::thread> workers;
    std::queue<std::function<void()>> tasks;
    std::mutex task_mutex;
    std::condition_variable task_cv;
    bool stop_workers;
    
    // æ¯ä¸ªæºçš„å½“å‰è¯»å–ä½ç½®
    std::vector<size_t> source_positions;
    // æ¯ä¸ªæºçš„æ‰€æœ‰è®°å½•
    std::vector<std::vector<DataRecord>> source_records;
    
    // ä¼˜å…ˆé˜Ÿåˆ—ç”¨äºå¤šè·¯å½’å¹¶
    std::priority_queue<QueueElement> merge_queue;
    
    // ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ç¼“å­˜
    bool download_file(const std::string& s3_path, const std::string& local_path);
    
    // ä»ç¼“å­˜è·å–æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»S3ä¸‹è½½
    std::vector<DataRecord> get_file_records(const std::string& s3_path);
    
    // è§£æParquetæ–‡ä»¶
    std::vector<DataRecord> parse_parquet_file(const std::string& file_path);
    
    // LRUç¼“å­˜ç®¡ç†
    void lru_cache_put(const std::string& key, const std::vector<DataRecord>& records);
    bool lru_cache_get(const std::string& key, std::vector<DataRecord>& records);
    void lru_evict();
    
    // çº¿ç¨‹æ± ä»»åŠ¡å¤„ç†
    void worker_thread();
    void enqueue_task(const std::function<void()>& task);
    
public:
    S3ParquetMerger(size_t cache_size = 10000, 
                   size_t threads = std::thread::hardware_concurrency(),
                   const std::string& cache_directory = "./s3_cache");
    
    ~S3ParquetMerger();
    
    // åˆå§‹åŒ–ï¼šæ·»åŠ éœ€è¦åˆå¹¶çš„æ–‡ä»¶
    void add_files(const std::vector<std::string>& s3_paths);
    
    // å‡†å¤‡åˆå¹¶ï¼šé¢„åŠ è½½éƒ¨åˆ†æ•°æ®
    void prepare_merge();
    
    // è·å–ä¸‹ä¸€ä¸ªåˆå¹¶åçš„è®°å½•
    bool get_next_record(DataRecord& record);
    
    // æ¸…ç†ç¼“å­˜
    void clean_cache();
};

#endif // S3_PARQUET_MERGER_H


### C++ å®ç° AWS S3 å¤šè·¯å½’å¹¶ Parquet æ•°æ®æ–¹æ¡ˆ

é’ˆå¯¹ä» AWS S3 è¯»å– Parquet æ–‡ä»¶å¹¶è¿›è¡Œå¤šè·¯å½’å¹¶çš„éœ€æ±‚ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½å®ç°æ–¹æ¡ˆï¼Œç»“åˆäº†ç¼“å­˜ç­–ç•¥ã€å¹¶è¡Œå¤„ç†å’Œé«˜æ•ˆå½’å¹¶ç®—æ³•ã€‚









### æ–¹æ¡ˆè§£æ

è¯¥å®ç°é‡‡ç”¨äº†å¤šå±‚æ¬¡ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤šè·¯å½’å¹¶æ“ä½œï¼š

1. **æ¶æ„è®¾è®¡**
   - é‡‡ç”¨ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å‹ï¼Œä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ–‡ä»¶ä¸‹è½½å’Œè§£æ
   - å®ç°äº†å†…å­˜LRUç¼“å­˜å’Œç£ç›˜ç¼“å­˜çš„äºŒçº§ç¼“å­˜æœºåˆ¶
   - ä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—å®ç°é«˜æ•ˆçš„å¤šè·¯å½’å¹¶

2. **æ€§èƒ½ä¼˜åŒ–ç‚¹**

   - **å¹¶è¡Œå¤„ç†**ï¼šåˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œä¸‹è½½å’Œè§£æParquetæ–‡ä»¶ï¼Œå……åˆ†åˆ©ç”¨ç½‘ç»œå¸¦å®½å’ŒCPUèµ„æº
   
   - **ç¼“å­˜ç­–ç•¥**ï¼š
     - å†…å­˜LRUç¼“å­˜å­˜å‚¨æœ€è¿‘è®¿é—®çš„æ–‡ä»¶æ•°æ®ï¼Œå‡å°‘é‡å¤è§£æå¼€é”€
     - ç£ç›˜ç¼“å­˜ä¿å­˜ä¸‹è½½çš„Parquetæ–‡ä»¶ï¼Œé¿å…é‡å¤ä»S3ä¸‹è½½
   
   - **é«˜æ•ˆå½’å¹¶**ï¼š
     - ä½¿ç”¨æœ€å°å †ï¼ˆä¼˜å…ˆé˜Ÿåˆ—ï¼‰å®ç°O(n log k)å¤æ‚åº¦çš„å¤šè·¯å½’å¹¶ï¼ˆkä¸ºæ–‡ä»¶æ•°ï¼‰
     - æ¯ä¸ªæ–‡ä»¶å†…éƒ¨å·²æ’åºï¼Œåªéœ€ç»´æŠ¤å½“å‰ä½ç½®æŒ‡é’ˆ
   
   - **æŒ‰éœ€åŠ è½½**ï¼šå¯ä»¥æ‰©å±•ä¸ºæ”¯æŒæ–‡ä»¶åˆ†ç‰‡åŠ è½½ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜

3. **å¯æ‰©å±•æ€§è€ƒè™‘**
   - å¯æ ¹æ®éœ€è¦è°ƒæ•´ç¼“å­˜å¤§å°å’Œçº¿ç¨‹æ•°é‡
   - æ”¯æŒåŠ¨æ€æ·»åŠ æ›´å¤šæ–‡ä»¶åˆ°å½’å¹¶é˜Ÿåˆ—
   - å¯æ‰©å±•ä¸ºæ”¯æŒèŒƒå›´æŸ¥è¯¢ï¼ŒåªåŠ è½½åŒ…å«ç›®æ ‡keyçš„æ–‡ä»¶éƒ¨åˆ†

